{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NH4NySJxysBK",
    "outputId": "f58e9c86-2f8a-4f34-f0da-895b87b01d22"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iasqt77xPhS",
    "outputId": "6e08cda4-bee2-4d12-d93e-9a56b64d0160"
   },
   "outputs": [],
   "source": [
    "# Tiny dataset\n",
    "# !gdown https://drive.google.com/uc?id=1qYPCnXXxjEcHEg3tLGt3fDkd2ialAgS4\n",
    "import os\n",
    "# Full dataset with jpeg\n",
    "!gdown https://drive.google.com/uc?id=1-xJoBvzwQKgJjPzHb3fq1sFicwyIisx7\n",
    "\n",
    "# Full dataset without jpeg\n",
    "# https://drive.google.com/file/d/1gFSdm8K9SXNPXG9tQWS4bmO_nappN2AL/view?usp=share_link\n",
    "# !gdown https://drive.google.com/uc?id=1gFSdm8K9SXNPXG9tQWS4bmO_nappN2AL\n",
    "!unzip data_v1.zip -d /content/data > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIv7rAFcxBAc",
    "outputId": "44f1052b-9e3f-4a3e-e7c2-33f88d6d6bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/data\n"
     ]
    }
   ],
   "source": [
    "%cd /content/data\n",
    "import json\n",
    "data = json.load(open(\"train_data.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHK3LRbmxBAf",
    "outputId": "4242c39e-9445-4ca0-94af-1cb3b9e1b0af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'image_id': 2,\n",
       " 'caption': 'ba chiếc thuyền đang di chuyển ở trên con sông',\n",
       " 'segment_caption': 'ba chiếc thuyền đang di_chuyển ở trên con sông'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['annotations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzycQeJCxBAh"
   },
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JdfutgM7y5Zk"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "vocab = json.load(open(\"/content/drive/MyDrive/IC/ResNet/vocab.json\", encoding = 'utf-8'))\n",
    "itos = list(vocab['stoi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lmCTfXpJy3T-"
   },
   "outputs": [],
   "source": [
    "def batch_decode(predictions_ids):\n",
    "    preds = []\n",
    "    for seq in predictions_ids:\n",
    "        preds.append(\" \".join([itos[id] for id in seq if id not in [0,1,2,3]]))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPFiuiE1xBAk"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hl1TN0HkxBAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "class IMCP_Test_Dataset(Dataset):\n",
    "  def __init__(self, image_path = \"public-test-images\", summary_path = \"test_data.json\"):\n",
    "    super().__init__()\n",
    "    self.data = json.load(open(summary_path, \"r\", encoding = 'utf-8'))\n",
    "    self.image_path = image_path\n",
    "    self.imgid2imgname = {entry['id']: entry['filename'] for entry in self.data['images']}\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data['images'])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    entry = self.data['images'][index]\n",
    "    image_id = entry['id']\n",
    "    image_name = entry['filename']\n",
    "    image = Image.open(os.path.join(self.image_path, image_name)).convert('RGB')\n",
    "    caption = [self.data['annotations'][i]['segment_caption'] for i in range(len(self.data['annotations'])) if self.data['annotations'][i]['image_id'] == image_id]\n",
    "    return image, caption, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0MBT-B8kxBAm"
   },
   "outputs": [],
   "source": [
    "test_dataset = IMCP_Test_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32D4TnloxBAn"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DigsYlWZxBAn"
   },
   "source": [
    "# Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsOtzaQ0xBAn"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "max_seq_length = 256\n",
    "class IMCP_Collator:\n",
    "  def __init__(self, vocab, train = True, model = \"resnet101\"):\n",
    "    self.vocab = vocab['stoi']\n",
    "    self.bos_id = self.vocab['<bos>']\n",
    "    self.eos_id = self.vocab['<eos>']\n",
    "    self.pad_id = self.vocab['<pad>']\n",
    "    self.model = model\n",
    "    self.train = train\n",
    "\n",
    "  def tokenize_texts(self, captions):\n",
    "    raw_captions = [caption.split(\" \") for caption in captions]\n",
    "    truncated_captions = [s[:max_seq_length] for s in raw_captions]\n",
    "    max_len = max([len(c) for c in truncated_captions])\n",
    "\n",
    "    padded_captions = []\n",
    "    for c in truncated_captions:\n",
    "        c = [self.vocab[word] for word in c]\n",
    "        seq = [self.bos_id] + c + [self.eos_id] + [self.pad_id] * (max_len - len(c))\n",
    "        padded_captions.append(seq)\n",
    "\n",
    "    padded_captions = [torch.Tensor(seq).long() for seq in padded_captions]\n",
    "    padded_captions = pad_sequence(padded_captions, batch_first=True)\n",
    "    return padded_captions\n",
    "\n",
    "  def resize_and_stack(self, images):\n",
    "    if self.model == \"resnet101\":\n",
    "      image_tensors = []\n",
    "      transform = transforms.Compose([\n",
    "          transforms.Resize((224, 224)),\n",
    "          transforms.ToTensor(),\n",
    "      ])\n",
    "\n",
    "      for image in images:\n",
    "        img_tensor = transform(image)\n",
    "        image.close()\n",
    "        image_tensors.append(img_tensor)\n",
    "\n",
    "      stacked = torch.stack(image_tensors)\n",
    "      return stacked\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    if self.train:\n",
    "      images = [example[0] for example in batch]\n",
    "      captions = [example[1] for example in batch]\n",
    "      return self.resize_and_stack(images), self.tokenize_texts(captions)\n",
    "    else:\n",
    "      images = [example[0] for example in batch]\n",
    "      captions = [example[1] for example in batch]\n",
    "      image_ids = [example[2] for example in batch]\n",
    "      return self.resize_and_stack(images), captions, image_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P5Q6uG7lxBAo"
   },
   "outputs": [],
   "source": [
    "collatorTest = IMCP_Collator(vocab, train = False, model = \"resnet101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JkPLQtmxBAp"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JgjrZhqLxBAq"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size = 16, collate_fn = collatorTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZU3E0AmxBAr"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmJJhv5hxBAr",
    "outputId": "9c61d049-44cd-47c4-d993-81616437920d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pre-trained RestNet101 model\n",
    "encoder = models.resnet101(pretrained=True).to(device)\n",
    "\n",
    "# Remove the last layer of the model\n",
    "modules = list(encoder.children())[:-1]\n",
    "encoder = nn.Sequential(*modules)\n",
    "\n",
    "# Freeze the parameters of the model\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define LSTM decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, feature_size, embed_size, hidden_size, vocab_size, num_layers, dropout=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + feature_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        captions = captions[:, :-1]\n",
    "        embeddings = self.embed(captions)\n",
    "        features = features.squeeze().unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
    "        embeddings = torch.cat((features, embeddings), 2)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, features, max_length):\n",
    "        batch_size = features.size(0)\n",
    "        input = torch.empty(batch_size, 1, device = device, dtype = torch.long).fill_(collatorTest.bos_id)\n",
    "        track_complete_example = torch.zeros(batch_size)\n",
    "        hidden = None\n",
    "        cell = None\n",
    "        # Loop through all the time steps\n",
    "        for t in range(max_length - 1):\n",
    "            embeddings = self.embed(input)\n",
    "            features2 = features.squeeze(-1).squeeze(-1).unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
    "            embeddings = torch.cat((features2, embeddings), 2)\n",
    "            if t == 0:\n",
    "                _, (hidden, cell) = self.lstm(embeddings)\n",
    "            else:\n",
    "                _, (hidden, cell) = self.lstm(embeddings, (hidden, cell))\n",
    "\n",
    "            pred = torch.argmax(self.linear(hidden), axis = -1)\n",
    "            input = torch.cat([input, pred.permute(1, 0)], dim = 1)\n",
    "            where_end = torch.where(pred == collatorTest.pad_id)[0]\n",
    "            track_complete_example[where_end] = 1\n",
    "            if track_complete_example.eq(1).all():\n",
    "              print(\"Early break in generate!\")\n",
    "              break\n",
    "\n",
    "        return input\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 10\n",
    "embed_size = 256\n",
    "feature_size = 2048\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab['stoi'].keys()) + 5\n",
    "\n",
    "# Tuning hyperparameters\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize decoder\n",
    "decoder = Decoder(feature_size, embed_size, hidden_size, vocab_size, num_layers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q-VmgWvxBAs"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31XttIPv2nrv",
    "outputId": "e830bd98-7123-450f-e466-d4baa101114d"
   },
   "outputs": [],
   "source": [
    "decoder.load_state_dict(torch.load('/content/drive/MyDrive/IC/ResNet/decoder.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PDDqzkg-uNhL"
   },
   "outputs": [],
   "source": [
    "my_preds = []\n",
    "targets = []\n",
    "data = []\n",
    "decoder.eval()\n",
    "encoder.eval()\n",
    "for i, (images, captions, image_ids) in enumerate(test_dataloader):\n",
    "    images = images.to(device)\n",
    "    # Forward pass\n",
    "    features = encoder(images)\n",
    "    out = decoder.generate(features, 30)\n",
    "    preds = out.detach().cpu().numpy()\n",
    "    preds = batch_decode(preds)\n",
    "    my_preds.extend(preds)\n",
    "    for pred, image_id in zip(preds, image_ids):\n",
    "        data.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"caption\": pred\n",
    "        })\n",
    "    targets.extend(captions)\n",
    "import json\n",
    "with open(\"result_resnetlstm.json\", \"w\") as file:\n",
    "    json.dump(data, file, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7N7n6EXcxBAt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def image_captioning(image_name: str):\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    plt.imshow(image)\n",
    "    images = collatorTest.resize_and_stack([image])\n",
    "    images = images.to(device)\n",
    "    features = encoder(images)\n",
    "    out = decoder.generate(features, 30)\n",
    "    preds = out.detach().cpu().numpy()\n",
    "    plt.title(batch_decode(preds)[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "V3eyqhIAxBAu",
    "outputId": "febf4ac5-06e1-4223-d08f-08aeca09c9c8"
   },
   "outputs": [],
   "source": [
    "image_captioning(\"public-test-images/00000000001.jpg\")\n",
    "image_captioning(\"public-test-images/00000000078.jpg\")\n",
    "image_captioning(\"public-test-images/00000000115.jpg\")\n",
    "image_captioning(\"public-test-images/00000000118.jpg\")\n",
    "image_captioning(\"public-test-images/00000000190.jpg\")\n",
    "image_captioning(\"public-test-images/00000000322.jpg\")\n",
    "image_captioning(\"public-test-images/00000000331.jpg\")\n",
    "image_captioning(\"public-test-images/00000000363.jpg\")\n",
    "image_captioning(\"public-test-images/00000000490.jpg\")\n",
    "image_captioning(\"public-test-images/00000000657.jpg\")\n",
    "image_captioning(\"public-test-images/00000000702.jpg\")\n",
    "image_captioning(\"public-test-images/00000000834.jpg\")\n",
    "image_captioning(\"public-test-images/00000001052.jpg\")\n",
    "image_captioning(\"public-test-images/00000001109.jpg\")\n",
    "image_captioning(\"public-test-images/00000009770.jpg\")\n",
    "image_captioning(\"public-test-images/00000011015.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
