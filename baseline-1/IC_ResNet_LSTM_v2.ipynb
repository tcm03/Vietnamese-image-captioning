{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HChDZSc6hZe0"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GStRpFQfYQmc",
    "outputId": "4aa8af7f-492f-4581-90c8-23d775e1ec3c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4ph0hgLY_VT",
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/drive/MyDrive/IC/ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7DFPf00Yxii",
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYAEq4UlhXVt",
    "outputId": "16234b7e-1990-4d76-e35e-055862b59998"
   },
   "outputs": [],
   "source": [
    "# Tiny dataset\n",
    "import os\n",
    "# Full dataset with jpeg\n",
    "!gdown https://drive.google.com/uc?id=1-xJoBvzwQKgJjPzHb3fq1sFicwyIisx7\n",
    "\n",
    "!unzip data_v1.zip -d /content/data > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_kx__wdBtvW"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(\"/content/data/train_data.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1U5QboMCJ11",
    "outputId": "abd28250-a228-41df-b6dd-d2cc0e1895cf"
   },
   "outputs": [],
   "source": [
    "data['annotations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXXEvVGCEXdy"
   },
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_SuiLm7foKT"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "def set_random_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhLXM7sCfpM-"
   },
   "outputs": [],
   "source": [
    "set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhEj46m3EXT8"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "from itertools import count\n",
    "\n",
    "class IMCP_Vocab():\n",
    "  def __init__(self, texts) -> None:\n",
    "    words = list(itertools.chain(*[text.split(\" \") for text in texts]))\n",
    "    counter = Counter(words)\n",
    "    self.vocab = {key: i for i, key in zip(count(start = 4), counter.keys())}\n",
    "    self.special_ids = [0, 1, 2, 3]\n",
    "    self.max_seq_len = 256\n",
    "    self.counter = counter\n",
    "    self.special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "    for id, token in zip(self.special_ids, self.special_tokens):\n",
    "      self.vocab[token] = id\n",
    "    self.vocab = {k: v for k, v in sorted(self.vocab.items(), key=lambda x:x[1])}\n",
    "    self.id2word = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    self.bos_token = \"<bos>\"\n",
    "    self.eos_token = \"<eos>\"\n",
    "    self.pad_token = \"<pad>\"\n",
    "    self.unk_token = \"<unk>\"\n",
    "\n",
    "  def get_vocab(self):\n",
    "    return self.vocab\n",
    "\n",
    "  def get_vocab_dump(self):\n",
    "    vocab = dict()\n",
    "    vocab['itos'] = list(vocab.keys())\n",
    "    vocab['stoi'] = self.vocab\n",
    "    vocab['freqs'] = dict(self.counter)\n",
    "    return vocab\n",
    "\n",
    "  def batch_decode(self, predictions_ids):\n",
    "    preds = []\n",
    "    for seq in predictions_ids:\n",
    "        preds.append(\" \".join([self.id2word[id] for id in seq if id not in [0,1,2,3,4,5]]))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuOXhFHpicmZ"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgcYI1_oh3x7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "class IMCP_Dataset(Dataset):\n",
    "  def __init__(self, image_path = \"/content/data/train-images\", summary_path = \"/content/data/train_data.json\", train = True):\n",
    "    super().__init__()\n",
    "    self.data = json.load(open(summary_path, \"r\"))\n",
    "    self.image_path = image_path\n",
    "    self.vocab = IMCP_Vocab(texts = [ann['segment_caption'] for ann in data['annotations']])\n",
    "    self.imgid2imgname = {entry['id']: entry['filename'] for entry in data['images']}\n",
    "    self.train = train\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data['annotations'])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    if self.train:\n",
    "      annotation = self.data['annotations'][index]\n",
    "      image_id = annotation['image_id']\n",
    "      image_name = self.imgid2imgname[image_id]\n",
    "      image = Image.open(os.path.join(self.image_path, image_name)).convert('RGB')\n",
    "      caption = annotation['segment_caption']\n",
    "      return image, caption\n",
    "    else:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZ0iUzSHGzf0"
   },
   "outputs": [],
   "source": [
    "train_dataset = IMCP_Dataset()\n",
    "vocab = train_dataset.vocab\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zClxA1twZC0U"
   },
   "outputs": [],
   "source": [
    "# Save vocab to file\n",
    "with open(\"/content/drive/MyDrive/IC/ResNet/vocab.json\", 'w+') as file:\n",
    "  json.dump(vocab.get_vocab_dump(), file, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FQqkd0ujbL3x"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM6PRtmcJXqj"
   },
   "source": [
    "# Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0kofjwdJY1H"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class IMCP_Collator:\n",
    "  def __init__(self, vocab, train = True):\n",
    "    self.vocab = vocab\n",
    "    self.bos_id = self.vocab.get_vocab()['<bos>']\n",
    "    self.eos_id = self.vocab.get_vocab()['<eos>']\n",
    "    self.pad_id = self.vocab.get_vocab()['<pad>']\n",
    "\n",
    "  def tokenize_texts(self, captions):\n",
    "    raw_captions = [caption.split(\" \") for caption in captions]\n",
    "    truncated_captions = [s[:self.vocab.max_seq_len] for s in raw_captions]\n",
    "    max_len = max([len(c) for c in truncated_captions])\n",
    "\n",
    "    padded_captions = []\n",
    "    for c in truncated_captions:\n",
    "        c = [self.vocab.get_vocab()[word] for word in c]\n",
    "        seq = [self.bos_id] + c + [self.eos_id] + [self.pad_id] * (max_len - len(c))\n",
    "        padded_captions.append(seq)\n",
    "\n",
    "    padded_captions = [torch.Tensor(seq).long() for seq in padded_captions]\n",
    "    padded_captions = pad_sequence(padded_captions, batch_first=True)\n",
    "    return padded_captions\n",
    "\n",
    "  def resize_and_stack(self, images):\n",
    "    # new_size = (224, 224)\n",
    "    image_tensors = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    for image in images:\n",
    "      img_tensor = transform(image)\n",
    "      image.close()\n",
    "      image_tensors.append(img_tensor)\n",
    "\n",
    "    stacked = torch.stack(image_tensors)\n",
    "    return stacked\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    images = [example[0] for example in batch]\n",
    "    captions = [example[1] for example in batch]\n",
    "    return {\n",
    "        'images': self.resize_and_stack(images),\n",
    "        'captions': self.tokenize_texts(captions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cZrPU1ztRh8K"
   },
   "outputs": [],
   "source": [
    "collator = IMCP_Collator(vocab, train = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPJbo7ZCSGcj"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beelDREDSD3x"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, collate_fn = collator, drop_last = True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = 16, collate_fn = collator, drop_last = True, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7RmhCUlOina"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZSQUdsXXhVM",
    "outputId": "8eea4f0e-3a0f-45e8-c834-139275eac058"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from transformers import get_scheduler, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load pre-trained RestNet101 model\n",
    "encoder = models.resnet101(pretrained=True).to(device)\n",
    "\n",
    "# Remove the last layer of the model\n",
    "modules = list(encoder.children())[:-1]\n",
    "encoder = nn.Sequential(*modules)\n",
    "\n",
    "# Freeze the parameters of the model\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize parameters\n",
    "def init(module):\n",
    "    for name, param in module.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_normal_(param.data)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "    return module\n",
    "\n",
    "# Define LSTM decoder\n",
    "class Decoder(nn.Module):\n",
    "     __init__(self, feature_size, embed_size, hidden_size, vocab_size, num_layers, dropout=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embeddidefng(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + feature_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        captions = captions[:, :-1]\n",
    "        embeddings = self.embed(captions)\n",
    "        features = features.squeeze().unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
    "        embeddings = torch.cat((features, embeddings), 2)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 10\n",
    "embed_size = 256\n",
    "feature_size = 2048\n",
    "hidden_size = 512\n",
    "vocab_size = len(collator.vocab.get_vocab()) + 5\n",
    "total_step = num_epochs * len(train_dataloader)\n",
    "\n",
    "# Tuning hyperparameters\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize decoder\n",
    "decoder = Decoder(feature_size, embed_size, hidden_size, vocab_size, num_layers, dropout).to(device)\n",
    "decoder.apply(init)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Creating optimizer and lr schedulers\n",
    "param_optimizer = list(decoder.parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer = torch.optim.AdamW(param_optimizer, lr=0.001, weight_decay=0.001)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = 0.1 * total_step, num_training_steps = total_step)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Start Epoch {epoch}\")\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # Move data to GPU\n",
    "        images = data['images'].to(device)\n",
    "        captions = data['captions'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        loss = criterion(outputs.permute(0, 2, 1), captions[:, 1:])\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Print loss\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_dataloader), loss.item()))\n",
    "\n",
    "    valid_loss = []\n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        # Move data to GPU\n",
    "        images = data['images'].to(device)\n",
    "        captions = data['captions'].to(device)\n",
    "        # Forward pass\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        loss = criterion(outputs.permute(0, 2, 1), captions[:, 1:])\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "    print('Epoch [{}/{}], Valid Loss: {:.4f}'.format(epoch+1, num_epochs, np.mean(valid_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8ohrPGcEz9VX"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), '/content/drive/MyDrive/IC/ResNet/encoder.pth')\n",
    "torch.save(decoder.state_dict(), '/content/drive/MyDrive/IC/ResNet/decoder.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
